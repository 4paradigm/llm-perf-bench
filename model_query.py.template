#! /usr/bin/env python3

import time
import json
import requests

class FailedQueryError(Exception):
    def get_err_msg(self):
        return str(self)

# Mock Adapter
def query_model(prompt, max_resp_tokens):
    for i in range(max_resp_tokens):
        time.sleep(0.01)
        yield '测试'

# # Adapter for huggingface Text-Generation-Inference
# def query_model(prompt, max_resp_tokens):
#     service_url = 'http://127.0.0.1:8080/generate_stream'
#     param_dict = {'inputs': prompt, 'parameters': {'max_new_tokens': max_resp_tokens, 'details': True}}
#     try:
#         response = requests.post(url=service_url, json=param_dict, stream=True)
#         for line in response.iter_lines(chunk_size = 8000):
#             if not line.startswith(b'data:'):
#                 continue
#             line_json = json.loads(line[5:])
#             token_text = line_json["token"]["text"]
#             if not line_json["token"]["special"]:
#                 yield token_text
#     except Exception as e:
#         raise FailedQueryError(str(e))

# # Adapter for OpenAPI (Completions API Legacy)
#def query_model(prompt, max_resp_tokens):
#    service_url = 'http://127.0.0.1:8085/v1/completions'
#    param_dict = {'model': 'baichuan', 'prompt': prompt, 'max_tokens': 512, 'temperature': 0, 'stream': True}
#    try:
#        response = requests.post(url=service_url, json=param_dict, stream=True)
#        for line in response.iter_lines(chunk_size = 8000):
#            if not line.startswith(b'data:'):
#                continue
#            line_json = json.loads(line[5:])
#            token_text = line_json["choices"][0]["text"]
#            if not line_json["choices"][0]["finish_reason"]:
#                yield token_text
#            else:
#                break
#    except Exception as e:
#        raise FailedQueryError(str(e))

# # Adapter for OpenAI Chat Completions API
# from openai import OpenAI
#
# def query_model(prompt, max_resp_tokens):
#     client = OpenAI(
#         base_url="http://127.0.0.1:8085/v1",
#         api_key="x",
#     )
#     try:
#         stream = client.chat.completions.create(
#             model="qwen-7b-chat",
#             max_tokens=max_resp_tokens,
#             messages=[
#                 # {"role": "system", "content": "You are a helpful assistant."},
#                 {"role": "user", "content": prompt}
#             ],
#             stream=True,
#             stop="<|endoftext|>",
#         )
#         for chunk in stream:
#             if not chunk.choices[0].finish_reason:
#                 token_text = chunk.choices[0].delta.content
#                 yield (token_text or "") # in case of empty returns
#             else:
#                 break
#     except Exception as e:
#         raise FailedQueryError(str(e))

if __name__ == '__main__':
    print('Use $python3 executor.py to test your query_model function')
