#! /usr/bin/env python3

import time
import json
import requests

class FailedQueryError(Exception):
    def get_err_msg(self):
        return str(self)

# Mock Adapter
def query_model(prompt, max_resp_tokens):
    for i in range(max_resp_tokens):
        time.sleep(0.01)
        yield '测试'

def prepare_messages(prompt):
    try:
        # 尝试解析成 JSON
        parsed = json.loads(prompt)
        # 判断是不是符合 messages 格式（list + dict里面有role和content）
        if isinstance(parsed, list) and all(isinstance(msg, dict) and 'role' in msg and 'content' in msg for msg in parsed):
            return parsed  # 是标准messages格式
    except (json.JSONDecodeError, TypeError):
        pass
    # 如果解析失败或者格式不对，当作一句话文本
    return [{'role': 'user', 'content': prompt}]

# def query_model(service_url, prompt, max_resp_tokens):
#     msgs = prepare_messages(prompt)

#     param_dict = {"model": "public/deepseek", "messages": msgs, "stream": True, "stop": "<JFSTOP>", "max_tokens": max_resp_tokens, "temperature": 0.001, "top_p": 0.8}
#     try:
#         response = requests.post(url=service_url, json=param_dict, stream=True)
#         for line in response.iter_lines(chunk_size = 128):
#             if not line.startswith(b'data:'):
#                 continue
#             try:
#                 line_json = json.loads(line[5:])
#                 token_text = line_json["choices"][0]["delta"].get("content")
#                 if token_text:
#                     yield token_text
#             except json.JSONDecodeError:
#                 pass
#     except Exception as e:
#         print(e)
#         raise FailedQueryError(str(e))


# # Adapter for huggingface Text-Generation-Inference
# def query_model(prompt, max_resp_tokens):
#     service_url = 'http://127.0.0.1:8080/generate_stream'
#     param_dict = {'inputs': prompt, 'parameters': {'max_new_tokens': max_resp_tokens, 'details': True}}
#     try:
#         response = requests.post(url=service_url, json=param_dict, stream=True)
#         for line in response.iter_lines(chunk_size = 8000):
#             if not line.startswith(b'data:'):
#                 continue
#             line_json = json.loads(line[5:])
#             token_text = line_json["token"]["text"]
#             if token_text and not line_json["token"]["special"]:
#                 yield token_text
#     except Exception as e:
#         raise FailedQueryError(str(e))

# # Adapter for OpenAI (Completions API Legacy)
#def query_model(prompt, max_resp_tokens):
#    service_url = 'http://127.0.0.1:8085/v1/completions'
#    param_dict = {'model': 'baichuan', 'prompt': prompt, 'max_tokens': 512, 'temperature': 0, 'stream': True}
#    try:
#        response = requests.post(url=service_url, json=param_dict, stream=True)
#        for line in response.iter_lines(chunk_size = 8000):
#            if not line.startswith(b'data:'):
#                continue
#             try:
#                 line_json = json.loads(line[5:])
#                 token_text = line_json["choices"][0].get("text")
#                 if token_text:
#                     yield token_text
#             except json.JSONDecodeError:
#                 pass
#    except Exception as e:
#        raise FailedQueryError(str(e))


# 注意：使用openai client在高并发下会有明显的性能衰减，暂时不明原因
# # Adapter for OpenAI Chat Completions API (Streaming)
# from openai import OpenAI
# def query_model(prompt, max_resp_tokens):
#     client = OpenAI(
#         base_url="http://127.0.0.1:8085/v1",
#         api_key="x",
#     )
#     try:
#         stream = client.chat.completions.create(
#             model="qwen-7b-chat",
#             max_tokens=max_resp_tokens,
#             messages=[
#                 # {"role": "system", "content": "You are a helpful assistant."},
#                 {"role": "user", "content": prompt}
#             ],
#             stream=True,
#             stop=["<|im_end|>","<|endoftext|>"],
#         )
#         for chunk in stream:
#             if not chunk.choices[0].finish_reason:
#                 # in case of empty returns
#                 token_text = chunk.choices[0].delta.content
#                 if token_text:
#                     yield token_text
#     except Exception as e:
#         raise FailedQueryError(str(e))

# 注意：使用openai client在高并发下会有明显的性能衰减，暂时不明原因
# # Adapter for OpenAI Chat Completions API
# from openai import OpenAI
# def query_model(prompt, max_resp_tokens):
#     client = OpenAI(
#         base_url="http://127.0.0.1:8085/v1",
#         api_key="x",
#     )
#     try:
#         response = client.chat.completions.create(
#             model="qwen-7b-chat",
#             max_tokens=max_resp_tokens,
#             messages=[
#                 {"role": "user", "content": prompt}
#             ],
#             stop=["<|im_end|>","<|endoftext|>"],
#         )
#         token_text = response.choices[0].message.content
#         if token_text:
#             yield token_text
#     except Exception as e:
#         raise FailedQueryError(str(e))

# Adapter for tensorRT-llm + triton-server 
# def query_model(prompt, max_resp_tokens):
#     service_url = 'http://127.0.0.1:8000/v2/models/ensemble/generate_stream'
#     param_dict = {"text_input": prompt, "max_tokens": max_resp_tokens, "bad_words": "", "stop_words": "", "end_id": [151643], "pad_id": [151643], "stream":True}
#     try:
#         response = requests.post(url=service_url, json=param_dict, stream=True)
#         for line in response.iter_lines(chunk_size = 8000):
#             if not line.startswith(b'data:'):
#                 continue
#             line_json = json.loads(line[5:])
#             token_text = line_json["text_output"]
#             if token_text:
#                 yield token_text
#     except Exception as e:
#         raise FailedQueryError(str(e))

if __name__ == '__main__':
    print('Use $python3 executor.py to test your query_model function')
