#! /usr/bin/env python3

import time
import json
import requests

class FailedQueryError(Exception):
    def get_err_msg(self):
        return str(self)

# Mock Adapter
def query_model(prompt, max_resp_tokens):
    for i in range(max_resp_tokens):
        time.sleep(0.01)
        yield '测试'

# # Adapter for huggingface Text-Generation-Inference
# def query_model(prompt, max_resp_tokens):
#     service_url = 'http://127.0.0.1:8080/generate_stream'
#     param_dict = {'inputs': prompt, 'parameters': {'max_new_tokens': max_resp_tokens, 'details': True}}
#     try:
#         response = requests.post(url=service_url, json=param_dict, stream=True)
#         for line in response.iter_lines(chunk_size = 8000):
#             if not line.startswith(b'data:'):
#                 continue
#             line_json = json.loads(line[5:])
#             token_text = line_json["token"]["text"]
#             if not line_json["token"]["special"]:
#                 yield token_text
#     except Exception as e:
#         raise FailedQueryError(str(e))

# # Adapter for OpenAI (Completions API Legacy)
#def query_model(prompt, max_resp_tokens):
#    service_url = 'http://127.0.0.1:8085/v1/completions'
#    param_dict = {'model': 'baichuan', 'prompt': prompt, 'max_tokens': 512, 'temperature': 0, 'stream': True}
#    try:
#        response = requests.post(url=service_url, json=param_dict, stream=True)
#        for line in response.iter_lines(chunk_size = 8000):
#            if not line.startswith(b'data:'):
#                continue
#            line_json = json.loads(line[5:])
#            token_text = line_json["choices"][0]["text"]
#            if not line_json["choices"][0]["finish_reason"]:
#                yield token_text
#            else:
#                break
#    except Exception as e:
#        raise FailedQueryError(str(e))

# # Adapter for OpenAI Chat Completions API (Streaming)
# from openai import OpenAI
# def query_model(prompt, max_resp_tokens):
#     client = OpenAI(
#         base_url="http://127.0.0.1:8085/v1",
#         api_key="x",
#     )
#     try:
#         stream = client.chat.completions.create(
#             model="qwen-7b-chat",
#             max_tokens=max_resp_tokens,
#             messages=[
#                 # {"role": "system", "content": "You are a helpful assistant."},
#                 {"role": "user", "content": prompt}
#             ],
#             stream=True,
#             stop=["<|im_end|>","<|endoftext|>"],
#         )
#         for chunk in stream:
#             if not chunk.choices[0].finish_reason:
#                 # in case of empty returns
#                 token_text = chunk.choices[0].delta.content or ""
#                 yield token_text 
#             else:
#                 break
#     except Exception as e:
#         raise FailedQueryError(str(e))

# # Adapter for OpenAI Chat Completions API
# from openai import OpenAI
# def query_model(prompt, max_resp_tokens):
#     client = OpenAI(
#         base_url="http://127.0.0.1:8085/v1",
#         api_key="x",
#     )
#     try:
#         response = client.chat.completions.create(
#             model="qwen-7b-chat",
#             max_tokens=max_resp_tokens,
#             messages=[
#                 {"role": "user", "content": prompt}
#             ],
#             stop=["<|im_end|>","<|endoftext|>"],
#         )
#         yield response.choices[0].message.content
#     except Exception as e:
#         raise FailedQueryError(str(e))

# Adapter for tensorRT-llm + triton-server 
# def query_model(prompt, max_resp_tokens):
#     service_url = 'http://127.0.0.1:8000/v2/models/ensemble/generate_stream'
#     param_dict = {"text_input": prompt, "max_tokens": max_resp_tokens, "bad_words": "", "stop_words": "", "end_id": [151643], "pad_id": [151643], "stream":True}
#     try:
#         response = requests.post(url=service_url, json=param_dict, stream=True)
#         for line in response.iter_lines(chunk_size = 8000):
#             if not line.startswith(b'data:'):
#                 continue
#             line_json = json.loads(line[5:])
#             token_text = line_json["text_output"]
#             yield token_text
#     except Exception as e:
#         raise FailedQueryError(str(e))

if __name__ == '__main__':
    print('Use $python3 executor.py to test your query_model function')
